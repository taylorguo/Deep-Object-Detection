  [å›¾è§£ç‰©ä½“æ£€æµ‹ & ç½‘ç»œæ¡†æ¶](https://github.com/taylorguo/Deep-Object-Detection/blob/master/assets/README.md)

Inspired by awesome object detection, deep object detection does a easy way for understanding in Chinese.

## ç›®å½•

- [å›¾è§£ç½‘ç»œæ¶æ„](#%E5%9B%BE%E8%A7%A3%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84)
  - [LeNet_AlexNet](#lenetalexnet)
  - [LeNet_AlexNet_Kerasä»£ç å®ç°](#lenetalexnetkeras%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0)
  - [VGG16ç½‘ç»œä¸ä»£ç å®ç°](#vgg16%E7%BD%91%E7%BB%9C%E4%B8%8E%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0)
  - [VGG19ç½‘ç»œä¸ä»£ç å®ç°](#vgg19%E7%BD%91%E7%BB%9C%E4%B8%8E%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0)
  - [Resnet](#resnet)
  - [ResNeXt:2016](#resnext2016)
  - [Xception:2016](#xception2016)
  - [SqueezeNet:2016](#squeezenet2016)
  - [DenseNet:2016](#densenet2016)
  - [MobileNet-v1:2017](#mobilenet-v12017)
  - [ShuffleNet:2017](#shufflenet2017)
  - [SENet : 2017](#senet--2017)
  - [MobileNet-V2:2018](#mobilenet-v22018)
  - [ShuffleNet-V2: 2018](#shufflenet-v2-2018)
- [å›¾è§£Object_Detectionæ¡†æ¶](#%E5%9B%BE%E8%A7%A3objectdetection%E6%A1%86%E6%9E%B6)
  - [Multi-stage Object Detection](#multi-stage-object-detection)
    - [RCNN : 2014](#rcnn--2014)
    - [SPPnet : 2014](#sppnet--2014)
    - [FCN : 2015](#fcn--2015)
    - [Fast R-CNN : 2015](#fast-r-cnn--2015)
    - [Faster R-CNN : 2015](#faster-r-cnn--2015)
    - [FPN : 2016](#fpn--2016)
    - [Mask R-CNN : 2017](#mask-r-cnn--2017)
  - [Single Stage Object Detection](#single-stage-object-detection)
    - [DenseBox : 2015](#densebox--2015)
    - [SSD : 2016](#ssd--2016)
    - [YoLov2 : 2016](#yolov2--2016)
    - [RetinaNet : 2017](#retinanet--2017)
    - [YoLov3 : 2018](#yolov3--2018)
    - [M2Det : 2019](#m2det--2019)
    - [CornerNet-Lite : 2019](#cornernet-lite--2019)
- [æ•°æ®é›†Object_Detection](#%E6%95%B0%E6%8D%AE%E9%9B%86objectdetection)
  - [General Dataset](#general-dataset)
  - [Animal](#animal)
  - [Plant](#plant)
  - [Food](#food)
  - [Transportation](#transportation)
  - [Scene](#scene)
  - [Face](#face)



# å›¾è§£ç½‘ç»œæ¶æ„

## LeNet_AlexNet
<img src="./assets/block_diagram/lenet_alexnet.png" width="600">

## LeNet_AlexNet_Kerasä»£ç å®ç°

[LeNet-Keras for mnist handwriting digital image classification](https://github.com/taylorguo/Deep-Object-Detection/blob/master/sample-code/network/lenet_keras.py)

LeNet-Keras restructure

<img src="./assets/code_diagram/lenet_revised.png" width="500">
Accuracy: 98.54%


===================================

[AlexNet-Keras for oxflower17 image classification](https://github.com/taylorguo/Deep-Object-Detection/blob/master/sample-code/network/alexnet_keras.py)

AlexNet-Keras restructure: ä¿®æ”¹åçš„ç½‘ç»œ val_acc: ~80%, è¿‡æ‹Ÿåˆ

<img src="./assets/code_diagram/alexnet_revised_v1.png" width="400">


===================================
## VGG16ç½‘ç»œä¸ä»£ç å®ç°

<img src="./assets/block_diagram/vgg16.png" width="800">

[VGG16 Keras å®˜æ–¹ä»£ç å®ç°](https://github.com/taylorguo/Deep-Object-Detection/blob/master/sample-code/network/vgg16.py)

[VGG16-Keras oxflower17 ç‰©ä½“åˆ†ç±»](https://github.com/taylorguo/Deep-Object-Detection/blob/master/sample-code/network/vgg16_keras.py): ä¿®æ”¹åçš„ç½‘ç»œ val_acc: ~86.4%, è¿‡æ‹Ÿåˆ

<img src="./assets/code_diagram/vgg16_tl.png" width="400">


## VGG19ç½‘ç»œä¸ä»£ç å®ç°

<img src="./assets/block_diagram/vgg19.png" width="800">

[VGG19 Keras å®˜æ–¹ä»£ç å®ç°](https://github.com/taylorguo/Deep-Object-Detection/blob/master/sample-code/network/vgg19.py)



## Resnet

- ResNet [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf) - CVPR
  
    - æ®‹å·®å—ä¸ç›´è¿å±‚:
  
        <img src="./assets/block_diagram/resnet_block.png" width="400">

    - æ®‹å·®ç½‘ç»œæ¶æ„:
   
        <img src="./assets/block_diagram/resnet_architecture.png" width="600">

    - æ®‹å·®ç½‘ç»œä¸­ Shortcut Connection å‚è€ƒæ–‡ç« 

        - 1995 - [Neural networks for pattern recognition - Bishop]()
        - 1996 - [Pattern recognition and neural networks - Ripley]()
        - 1999 - [Modern applied statistics with s-plus - Venables & Ripley]()


- [Highway Networks](https://arxiv.org/pdf/1505.00387v2.pdf), [ä¸­æ–‡ç¿»è¯‘å‚è€ƒ](https://www.cnblogs.com/2008nmj/p/9104744.html)

- [Convolutional Neural Networks at Constrained Time Cost](https://arxiv.org/pdf/1412.1710.pdf)

    - å®éªŒè¡¨æ˜: åŠ æ·±ç½‘ç»œ, ä¼šå‡ºç°è®­ç»ƒè¯¯å·®

===================================


## ResNeXt:2016

- [ResNeXt](https://arxiv.org/pdf/1611.05431.pdf): Aggregated Residual Transformations for Deep Neural Networks


## Xception:2016

- [Xception](https://arxiv.org/pdf/1610.02357.pdf): Deep Learning with Depthwise Separable Convolutions


## SqueezeNet:2016

- [SqueezeNet](https://arxiv.org/pdf/1602.07360.pdf): AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size


## DenseNet:2016

- [DenseNet](https://arxiv.org/pdf/1608.06993.pdf) : Densely Connected Convolutional Networks
  
- [DenseNet- Github](https://github.com/liuzhuang13/DenseNet#results-on-imagenet-and-pretrained-models)
    - Dense Block å±‚é—´é“¾æ¥é‡‡ç”¨concat, è€Œä¸æ˜¯æŒ‰å…ƒç´ add





## MobileNet-v1:2017

- [MobileNets](https://arxiv.org/pdf/1704.04861.pdf) : Efficient Convolutional Neural Networks for Mobile Vision Applications



## ShuffleNet:2017

- [ShuffleNet](https://arxiv.org/pdf/1707.01083.pdf): An Extremely Efficient Convolutional Neural Network for Mobile Devices
  
- å›¾è§£ShuffleNetå•å…ƒå—:

    <img src="./assets/block_diagram/shufflenet.png" width="600">

- Code:
  - [ShuffleNet Tensorflow](https://github.com/MG2033/ShuffleNet)


## SENet : 2017

- [SENet](https://arxiv.org/pdf/1709.01507.pdf) Squeeze-and-Excitation Networks


## MobileNet-V2:2018

- [MobileNetV2 ](https://arxiv.org/pdf/1801.04381.pdf): Inverted Residuals and Linear Bottlenecks


## ShuffleNet-V2: 2018

- [ShuffleNet V2](https://arxiv.org/pdf/1807.11164.pdf): Practical Guidelines for Efficient CNN Architecture Design

=============================

# [å›¾è§£Object_Detectionæ¡†æ¶](https://github.com/taylorguo/Deep-Object-Detection/blob/master/assets/README.md)

é€šç”¨æ–‡æ¡£

- [cs231n : Spatial Localization and Detection](http://cs231n.stanford.edu/slides/2016/winter1516_lecture8.pdf)


2010

- [Object Detection with Discriminatively Trained Part Based Models](http://cs.brown.edu/people/pfelzens/papers/lsvm-pami.pdf)


2011

- [Ensemble of Exemplar-SVMs for Object Detection and Beyond](http://www.cs.cmu.edu/~efros/exemplarsvm-iccv11.pdf)


2013 

- [OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks](https://arxiv.org/pdf/1312.6229.pdf)

    - [Code](https://github.com/sermanet/OverFeat)

    - sliding window detector on an image pyramid


2014

- [VGG: Very Deep Convolutional Networks for Large-Scale Image Recognition](http://www.arxiv.org/pdf/1409.1556.pdf)

- SPP: [Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition](https://arxiv.org/pdf/1406.4729.pdf)



2017

- [On the Origin of Deep Learning](https://arxiv.org/pdf/1702.07800.pdf)

2018

- [A guide to convolution arithmetic for deep learning](https://arxiv.org/pdf/1603.07285.pdf)


- [Progressive Neural Architecture Search](https://arxiv.org/pdf/1712.00559.pdf)




===========================

## Multi-stage Object Detection






###  RCNN : 2014

  - [Region-Based Convolutional Networks for Accurate Object Detection and Segmentation](http://medialab.sjtu.edu.cn/teaching/CV/hw/related_papers/3_detection.pdf)

  - v5 [Rich feature hierarchies for accurate object detection and semantic segmentation](https://arxiv.org/pdf/1311.2524v3.pdf) - CVPR

  - region proposal with scale-normalized before classifying with a ConvNet

    <img src="./assets/algorithm/rcnn.png" width="600">

  - [RCNN Keras Code](https://github.com/yhenon/keras-rcnn)

    <img src="./assets/algorithm/RCNN_algorithm.png" width="600">
    


###  SPPnet : 2014

- SPPnet [Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition](https://arxiv.org/pdf/1406.4729.pdf) - ECCV
    <img src="./assets/algorithm/sppnet.png" width="600">

    - [ROI Pooling ](http://wavelab.uwaterloo.ca/wp-content/uploads/2017/04/Lecture_6.pdf)





### FCN : 2015

- FCN -[Fully convolutional networks for semantic segmentation](https://arxiv.org/pdf/1411.4038.pdf) - CVPR
    - å…¨å·ç§¯ç½‘ç»œå°†æœ€åçš„ä¸‰å±‚å…¨è¿æ¥å±‚, ç”¨å¤šé€šé“åŒå°ºå¯¸å·ç§¯æ ¸, è½¬æ¢æˆå·ç§¯å±‚; ä½¿è¾“å…¥å›¾åƒå°ºå¯¸å¯ä»¥æ”¹åŠ¨

        <img src="./assets/block_diagram/fcn_architecture.png" width="400">

    - è¯­ä¹‰åˆ†å‰²çš„ç½‘ç»œç»“æ„:  
      - æå–ä¸åŒçš„æ± åŒ–å±‚ç‰¹å¾å›¾, å¯¹ç‰¹å¾å›¾è¿›è¡Œä¸Šé‡‡æ ·
      - ä¸Šé‡‡æ ·ä½¿ç”¨åå·ç§¯(è½¬ç½®å·ç§¯) : å¯¼è‡´åå·ç§¯åçš„å›¾åƒä¸å¤Ÿç»†è‡´
      - è·³å±‚ç»“æ„, ç‰¹å¾å›¾èåˆ: å…ƒç´ æŒ‰åƒç´ ç›¸åŠ (Kerasé‡Œé¢ add å‡½æ•°)
      - å°†ç‰¹å¾å›¾è½¬æ¢æˆåŸå›¾åƒå¤§å°è¿›è¡Œåƒç´ é¢„æµ‹

        <img src="./assets/block_diagram/fcn_upooling.jpg" width="400">

        <img src="./assets/block_diagram/fcn.png" width="400">

    - è¯­ä¹‰åˆ†å‰²çš„é—®é¢˜å®šä¹‰:
      - åƒç´ å€¼äºŒåˆ†ç±»
      - æœ€åä¸€å±‚å·ç§¯ä¸º1x1x21(VOC 20ç±»ç‰©ä½“+1ç±»èƒŒæ™¯)

        <img src="./assets/block_diagram/fcn_block.png" width="400">

        [å‚è€ƒèµ„æ–™: å…¨å·ç§¯ç½‘ç»œ FCN è¯¦è§£](https://blog.csdn.net/sinat_24143931/article/details/78696442)

        [å‚è€ƒèµ„æ–™: 10åˆ†é’Ÿçœ‹æ‡‚FCN: è¯­ä¹‰åˆ†å‰²æ·±åº¦æ¨¡å‹å…ˆé©±](http://www.sohu.com/a/270896638_633698)

    - code:
      - [FCN in tensorflow](https://github.com/MarvinTeichmann/tensorflow-fcn)
      - [FCN offical](https://github.com/shelhamer/fcn.berkeleyvision.org)


### Fast R-CNN : 2015

- [Fast R-CNN](https://arxiv.org/pdf/1504.08083.pdf) - ICCV

    <img src="./assets/algorithm/fast_rcnn.png" width="600">

### Faster R-CNN : 2015

- [Faster R-CNN: To- wards real-time object detection with region proposal net- works](https://arxiv.org/pdf/1506.01497.pdf) - NIPS

    - RPN(Region Proposal Network) & Anchor Box

    <img src="./assets/algorithm/faster_rcnn_v2.png">

    - [Convolutional Feature Maps](http://kaiminghe.com/iccv15tutorial/iccv2015_tutorial_convolutional_feature_maps_kaiminghe.pdf)


- ç‰©ä½“æ£€ç´¢ [Faster R-CNN Features for Instance Search](https://arxiv.org/pdf/1604.08893.pdf) 





### FPN : 2016

- [Feature Pyramid Networks for Object Detection](https://arxiv.org/pdf/1612.03144.pdf)

    - Idea from traditional CV feature pyramids, for compute and memory intensive in DL 

        æƒ³æ³•æºè‡ªä¼ ç»Ÿè®¡ç®—æœºè§†è§‰ä¸­çš„ç‰¹å¾é‡‘å­—å¡”, æ·±åº¦å­¦ä¹ ä¸­æ²¡ç”¨æ˜¯å› ä¸ºè®¡ç®—å¯†é›†,å å†…å­˜

    - bottome-up in FeedForward: deepest layer of each stage should have the strongest features
    
        æ¯é˜¶æ®µçš„æœ€æ·±çš„ä¸€å±‚åº”è¯¥æœ‰æœ€å¼ºçš„ç‰¹å¾

    <img src="./assets/algorithm/fpn.png">

    - [å‚è€ƒæ–‡æ¡£: Understanding FPN](https://medium.com/@jonathan_hui/understanding-feature-pyramid-networks-for-object-detection-fpn-45b227b9106c)
  
    - Code:
      - [FPN in Mask-RCNN Keras Code](https://github.com/matterport/Mask_RCNN/blob/master/mrcnn/model.py)
      - [FPN in Tensorflow](https://github.com/yangxue0827/FPN_Tensorflow)
      - [FPN in Caffe](https://github.com/unsky/FPN)



### Mask R-CNN : 2017 

- [Mask R-CNN](https://arxiv.org/pdf/1703.06870.pdf)
  - Code:
    - [Keras matterport](https://github.com/matterport/Mask_RCNN)
    - [Caffe2 Facebook](https://github.com/facebookresearch/Detectron)
    - [PyTorch wannabeOG](https://github.com/wannabeOG/Mask-RCNN)
    - [MXNet TuSimple](https://github.com/TuSimple/mx-maskrcnn)
    - [Chainer DeNA](https://github.com/DeNA/Chainer_Mask_R-CNN)



============================
## Single Stage Object Detection


### DenseBox : 2015

- [DenseBox: Unifying Landmark Localization with End to End Object Detection](https://arxiv.org/pdf/1509.04874.pdf)

### SSD : 2016

- [SSD: Single Shot MultiBox Detector](https://arxiv.org/pdf/1512.02325.pdf) - ECCV

    - å·¥ä½œæµç¨‹:

        - ç‰¹å¾æå–ç½‘ç»œä¸ºVGG-16, è¾¹ç•Œæ¡† å’Œ åˆ†ç±» ä¸ºç‰¹å¾å›¾é‡‘å­—å¡”
    
    - ç½‘ç»œæ¶æ„: 

        <img src="./assets/block_diagram/SSD-architecture.png" width="600">

    - æŸå¤±å‡½æ•°:

        - ä½ç½®Smooth L1 Loss å’Œ å¤šåˆ†ç±»Softmax çš„å’Œ

             <img src="./assets/block_diagram/SSD-framework.png" width="600">


### YoLov2 : 2016

- YOLOv2 [YOLO9000: Better, Faster, Stronger](https://arxiv.org/pdf/1612.08242.pdf)

    - å·¥ä½œæµç¨‹ï¼š

        - åœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šé¢„è®­ç»ƒ CNNç½‘ç»œ

        - å›¾åƒæ‹†åˆ†ä¸ºå•å…ƒæ ¼, å¦‚æœä¸€ä¸ªå¯¹è±¡çš„ä¸­å¿ƒåœ¨ä¸€ä¸ªå•å…ƒæ ¼å†…ï¼Œè¯¥å•å…ƒæ ¼å°±â€œè´Ÿè´£â€æ£€æµ‹è¯¥å¯¹è±¡
            
            æ¯ä¸ªå•å…ƒé¢„æµ‹ï¼ˆaï¼‰è¾¹ç•Œæ¡†ä½ç½®ï¼Œï¼ˆbï¼‰ç½®ä¿¡åº¦åˆ†æ•°ï¼Œï¼ˆcï¼‰ä»¥è¾¹ç•Œæ¡†ä¸­çš„å¯¹è±¡çš„å­˜åœ¨ä¸ºæ¡ä»¶çš„å¯¹è±¡ç±»çš„æ¦‚ç‡

        - ä¿®æ”¹é¢„è®­ç»ƒçš„CNNçš„æœ€åä¸€å±‚ä»¥è¾“å‡ºé¢„æµ‹å¼ é‡
    
    - ç½‘ç»œæ¶æ„:

        <img src="./assets/block_diagram/yolo-network-architecture.png" width="600">

    - æŸå¤±å‡½æ•°:

        - 2éƒ¨åˆ†ç»„æˆ: è¾¹ç•Œæ¡†å›å½’ å’Œ åˆ†ç±»æ¡ä»¶æ¦‚ç‡ - éƒ½é‡‡ç”¨å¹³æ–¹å·®çš„å’Œ

             <img src="./assets/block_diagram/yolo-responsible-predictor.png"  width="400">


### RetinaNet : 2017

- RetinaNet:[Focal Loss for Dense Object Detection](https://arxiv.org/pdf/1708.02002.pdf)

    - å·¥ä½œæµç¨‹:

        - ç„¦ç‚¹æŸå¤±ä¸ºæ˜æ˜¾çš„,å®¹æ˜“é”™è¯¯åˆ†ç±»çš„æƒ…å†µ(å…·æœ‰å™ªå£°çº¹ç†æˆ–éƒ¨åˆ†å¯¹è±¡çš„èƒŒæ™¯)åˆ†é…æ›´å¤šæƒé‡ï¼Œå¹¶ä¸”é™ä½ç®€å•æƒ…å†µæƒé‡(æ˜æ˜¾ç©ºç™½èƒŒæ™¯)
        
        - ç‰¹å¾æå–ç½‘ç»œä¸ºResNet, ç‰¹å¾é‡‘å­—å¡”æé«˜æ£€æµ‹æ€§èƒ½

            <img src="./assets/block_diagram/featurized-image-pyramid.png" width="600">

    - ç½‘ç»œæ¶æ„:

        <img src="./assets/block_diagram/retina-net.png" width="600">

    


### YoLov3 : 2018

- [YOLOv3: An Incremental Improvement](https://arxiv.org/pdf/1804.02767.pdf)

    - bbox é¢„æµ‹ä½¿ç”¨å°ºå¯¸èšç±»

        - æ¯ä¸ªboxæœ‰4ä¸ªåæ ‡

        - è®­ç»ƒæ—¶, ä½¿ç”¨è¯¯å·®å¹³æ–¹å’ŒæŸå¤±å‡½æ•° sum of squared error loss

        - bbox objectåˆ†å€¼, ç”¨ logistic regression

        - åˆ†ç±»å™¨ ä½¿ç”¨ logistic regression, æŸå¤±å‡½æ•°binary cross-entropy

    - å€Ÿé‰´äº† FPN ç½‘ç»œ

    - ç‰¹å¾æå–å·ç§¯ç½‘ç»œ

        - 3x3, 1x1 å·ç§¯å±‚äº¤æ›¿

        - å€Ÿé‰´äº† ResNet, ä½¿ç”¨äº†ç›´è¿, åˆ†åˆ«ä»å·ç§¯å±‚æˆ–ç›´è¿å±‚è¿›è¡Œç›´è¿


### M2Det : 2019 

- [M2Det](https://arxiv.org/pdf/1811.04533.pdf)


### CornerNet-Lite : 2019

- [CornerNet-Lite](https://arxiv.org/pdf/1904.08900.pdf) : Efficient Keypoint Based Object Detection
  - CornerNet-Saccade: å¤„ç†ç‰¹å¾å›¾çš„åƒç´ , ä¸€ä¸ªè£å‰ªå¤šä¸ªæ£€æµ‹; ç¦»çº¿å¤„ç†
  - CornetNet-Squeeze: éª¨å¹²ç½‘ç»œ, ä½¿ç”¨SqueezeNet, æ²™æ¼æ¶æ„; å®æ—¶å¤„ç†



[å‚è€ƒèµ„æ–™: ç›®æ ‡æ£€æµ‹ç®—æ³•æ€»ç»“](https://www.cnblogs.com/guoyaohua/p/8994246.html)


=============================

# æ•°æ®é›†Object_Detection

ä¸ç¡®å®šæ¯ä¸ªæ•°æ®é›†éƒ½åŒ…å«å®Œæ•´çš„ç‰©ä½“æ£€æµ‹æ•°æ®æ ‡æ³¨ã€‚

## General Dataset

- [æ•°æ®é›†æ”¶é›† Dataset Collection](http://www.escience.cn/people/lichang/Data.html)

- [æ•°æ®é›†: 25ç§ç®€ä»‹](https://www.easemob.com/news/1433)

- [ImageNet æœ€å¤§çš„å›¾åƒè¯†åˆ«å›¾åƒåº“](http://www.image-net.org/)

    - 14,197,122å¼ å›¾åƒ

- [PASCAL Visual Object Classes Challenge 2008 (VOC2008)](http://host.robots.ox.ac.uk/pascal/VOC/voc2008/htmldoc/voc.html), [VOC-2012](http://pjreddie.com/projects/pascal-voc-dataset-mirror/)


- [Open Images dataset(å¸¦æ ‡æ³¨)](https://github.com/openimages/dataset)


    - è¿‘900ä¸‡ä¸ªå›¾åƒURLæ•°æ®é›†, æ•°åƒä¸ªç±»çš„å›¾åƒçº§æ ‡ç­¾è¾¹æ¡†å¹¶ä¸”è¿›è¡Œäº†æ ‡æ³¨ã€‚
    
    - æ•°æ®é›†åŒ…å«9,011,219å¼ å›¾åƒçš„è®­ç»ƒé›†, 41,260å¼ å›¾åƒçš„éªŒè¯é›†, 125,436å¼ å›¾åƒçš„æµ‹è¯•é›†ã€‚


- [Corel5K å›¾åƒé›†](https://github.com/watersink/Corel5K)

    - Corel5Kå›¾åƒé›†ï¼Œå…±5000å¹…å›¾ç‰‡ï¼ŒåŒ…å«50ä¸ªè¯­ä¹‰ä¸»é¢˜ï¼Œæœ‰å…¬å…±æ±½è½¦ã€æé¾™ã€æµ·æ»©ç­‰ã€‚





## Animal


[Stanford Dogs ğŸ¶ Dataset : Over 20,000 images of 120 dog breeds](https://www.kaggle.com/jessicali9530/stanford-dogs-dataset)


- Context

    The Stanford Dogs dataset contains images of 120 breeds of dogs from around the world. This dataset has been built using images and annotation from ImageNet for the task of fine-grained image categorization. It was originally collected for fine-grain image categorization, a challenging problem as certain dog breeds have near identical features or differ in colour and age.

    æ¥æºäºimagenet, ç”¨äºå›¾åƒç»†ç²’åº¦åˆ†ç±»


- Content

    - Number of categories: 120
    - Number of images: 20,580
    - Annotations: Class labels, Bounding boxes


[Honey Bee pollen : High resolution images of individual bees on the ramp](https://www.kaggle.com/ivanfel/honey-bee-pollen)

- Context

    This image dataset has been created from videos captured at the entrance of a bee colony in June 2017 at the Bee facility of the Gurabo Agricultural Experimental Station of the University of Puerto Rico.
    
    è¯†åˆ« èœœèœ‚ ğŸ æˆç²‰ æˆ–è€… æœªæˆç²‰

- Content

    - images/ contains images for pollen bearing and no pollen bearing honey bees.

        - The prefix of the images names define their class: e.g. NP1268-15r.jpg for non-pollen and P7797-103r.jpg for pollen bearing bees. 
        - The numbers correspond to frame and item number respectively, you need to be careful that they are not numbered sequentially.



    - Read-skimage.ipynb Jupyter notebook for simple script to load the data and create the dataset using skimage library.




## Plant

[Flowers Recognition : This dataset contains labeled 4242 images of flowers.](https://www.kaggle.com/alxmamaev/flowers-recognition)

- Context

    This dataset contains 4242 images of flowers. The data collection is based on the data flicr, google images, yandex images. You can use this datastet to recognize plants from the photo.

    

- Content

    - five classes: chamomile, tulip, rose, sunflower, dandelion
    - each class there are about 800 photos
    - resolution: about 320x240 pixels


[VGG - 17 Category Flower Dataset](http://www.robots.ox.ac.uk/~vgg/data/flowers/17/index.html)

- Context

    - 17 category flower dataset with 80 images for each class
    - 80 images for each category
    

- Content

    - The datasplits used in this paper are specified in datasplits.mat

    - There are 3 separate splits. The results in the paper are averaged over the 3 splits.

    - Each split has a training file (trn1,trn2,trn3), a validation file (val1, val2, val3) and a testfile (tst1, tst2 or tst3). 


[VGG - 102 Category Flower Dataset](http://www.robots.ox.ac.uk/~vgg/data/flowers/102/index.html)

- Context

    - 102 category dataset, consisting of 102 flower categories
    - Each class consists of between 40 and 258 images
    

- Content

    - The datasplits used in this paper are specified in setid.mat.

    - The results in the paper are produced on a 103 category database. - - The two categories labeled Petunia have since been merged since they are the same.
    - There is a training file (trnid), a validation file (valid) and a testfile (tstid).



[Fruits 360 dataset : A dataset with 65429 images of 95 fruits](https://www.kaggle.com/moltean/fruits)

- Context

    The following fruits are included: Apples (different varieties: Golden, Red Yellow, Granny Smith, Red, Red Delicious), Apricot, Avocado, Avocado ripe, Banana (Yellow, Red, Lady Finger), Cactus fruit, Cantaloupe (2 varieties), Carambula, Cherry (different varieties, Rainier), Cherry Wax (Yellow, Red, Black), Chestnut, Clementine, Cocos, Dates, Granadilla, Grape (Blue, Pink, White (different varieties)), Grapefruit (Pink, White), Guava, Hazelnut, Huckleberry, Kiwi, Kaki, Kumsquats, Lemon (normal, Meyer), Lime, Lychee, Mandarine, Mango, Mangostan, Maracuja, Melon Piel de Sapo, Mulberry, Nectarine, Orange, Papaya, Passion fruit, Peach (different varieties), Pepino, Pear (different varieties, Abate, Kaiser, Monster, Williams), Physalis (normal, with Husk), Pineapple (normal, Mini), Pitahaya Red, Plums (different varieties), Pomegranate, Pomelo Sweetie, Quince, Rambutan, Raspberry, Redcurrant, Salak, Strawberry (normal, Wedge), Tamarillo, Tangelo, Tomato (different varieties, Maroon, Cherry Red), Walnut.

    
- Content

    - Total number of images: 65429.
        - Training set size: 48905 images (one fruit per image).
        - Test set size: 16421 images (one fruit per image).
        - Multi-fruits set size: 103 images (more than one fruit (or fruit class) per image)
    - Number of classes: 95 (fruits).
    - Image size: 100x100 pixels.


- [GitHub download: Fruits-360 dataset](https://github.com/Horea94/Fruit-Images-Dataset)



[Plant Seedlings Classification : Determine the species of a seedling from an image](https://www.kaggle.com/c/plant-seedlings-classification)

- Context

    - a dataset containing images of approximately 960 unique plants belonging to 12 species at several growth stages

- Content

    - [A Public Image Database for Benchmark of Plant Seedling Classification Algorithms](https://arxiv.org/abs/1711.05458)


[V2 Plant Seedlings Dataset : Images of crop and weed seedlings at different growth stages](https://www.kaggle.com/vbookshelf/v2-plant-seedlings-dataset)


- Context
    - The V1 version of this dataset was used in the Plant Seedling Classification playground competition here on Kaggle. This is the V2 version. Some samples in V1 contained multiple plants. The datasetâ€™s creators have now removed those samples.

- Content

    - This dataset contains 5,539 images of crop and weed seedlings. 
    - The images are grouped into 12 classes as shown in the above pictures. These classes represent common plant species in Danish agriculture. Each class contains rgb images that show plants at different growth stages. 
    - The images are in various sizes and are in png format.





## Food

[UEC Food-256 Japan Food](http://foodcam.mobi/dataset256.html)

- Context

    - The dataset "UEC FOOD 256" contains 256-kind food photos. Each food photo has a bounding box indicating the location of the food item in the photo. 

    - Most of the food categories in this dataset are popular foods in Japan and other countries. 


- Content 

    - [1-256] : directory names correspond to food ID.
    - [1-256]/*.jpg : food photo files (some photos are duplicated in two or more directories, since they includes two or more food items.)
    - [1-256]/bb_info.txt: bounding box information for the photo files in each directory

    - category.txt : food list including the correspondences between food IDs and food names in English
    - category_ja.txt : food list including the correspondences between food IDs and food names in Japanese
    - multiple_food.txt: the list representing food photos including two or more food items

[FoodDD: Food Detection Dataset](http://www.site.uottawa.ca/~shervin/food/), [è®ºæ–‡](http://www.site.uottawa.ca/~shervin/pubs/FoodRecognitionDataset-MadiMa.pdf)

[NutriNet: A Deep Learning Food and Drink Image Recognition System for Dietary Assessment](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5537777/)

[ChineseFoodNet: A large-scale Image Dataset for Chinese Food Recognition - 2017](https://arxiv.org/pdf/1705.02743.pdf)

[Yummly-28K - 2017](http://isia.ict.ac.cn/dataset/)

    - Content 
    
        - 27,638 recipes in total. 
        - Each recipe contains one recipe image, the ingredients, the cuisine and the course information.     
        - There are 16 kinds of cuisines (e.g,â€œAmericanâ€,â€œItalianâ€ and â€œMexicanâ€) 
        - and 13 kinds of recipe courses (e.g, â€œMain Dishesâ€,â€œDessertsâ€ and â€œLunch and Snacksâ€).

[VireoFood-172 dataset](http://vireo.cs.cityu.edu.hk/vireofood172/),   [è®ºæ–‡-2016](http://vireo.cs.cityu.edu.hk/jingjing/papers/chen2016deep.pdf)

[Dishes: a restaurant-oriented food dataset - 2015](http://isia.ict.ac.cn/dataset/Geolocation-food/)




## Transportation


[Boat types recognition : About 1,500 pictures of boats classified in 9 categories](https://www.kaggle.com/clorichel/boat-types-recognition)

- Context

    This dataset is used on this blog post https://clorichel.com/blog/2018/11/10/machine-learning-and-object-detection/ where you'll train an image recognition model with TensorFlow to find about anything on pictures and videos.

    

- Content

    1,500 pictures of boats, of various sizes, but classified by those different types: buoy, cruise ship, ferry boat, freight boat, gondola, inflatable boat, kayak, paper boat, sailboat.





## Scene


[Intel Image Classification : Image Scene Classification of Multiclass](https://www.kaggle.com/puneet6060/intel-image-classification)

- Context

    image data of Natural Scenes around the world

    

- Content

    - This Data contains around 25k images of size 150x150 distributed under 6 categories. {'buildings' -> 0, 'forest' -> 1, 'glacier' -> 2, 'mountain' -> 3, 'sea' -> 4, 'street' -> 5 }

    - The Train, Test and Prediction data is separated in each zip files. There are around 14k images in Train, 3k in Test and 7k in Prediction. This data was initially published on https://datahack.analyticsvidhya.com by Intel to host a Image classification Challenge.






## Face 

[CelebFaces Attributes (CelebA) Dataset : Over 200K images of celebrities with 40 binary attribute annotations](https://www.kaggle.com/jessicali9530/celeba-dataset/version/2)

